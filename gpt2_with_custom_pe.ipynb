{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06f164cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 17:09:29.068478: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747913969.083039   60723 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747913969.087490   60723 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747913969.099832   60723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747913969.099850   60723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747913969.099851   60723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747913969.099853   60723 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-22 17:09:29.103585: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2LMHeadModel, GPT2Config, AutoTokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "from typing import Optional, Tuple\n",
    "from einops import rearrange # For easier tensor manipulation\n",
    "import math\n",
    "import logging\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- 1. RoPE (Rotary Positional Embeddings) Implementation ---\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1] // 2], x[..., x.shape[-1] // 2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids):\n",
    "    # q, k: (batch_size, num_heads, seq_len, head_dim)\n",
    "    # cos, sin: (seq_len, head_dim) or (1, seq_len, 1, head_dim) depending on broadcasting\n",
    "    # position_ids: (batch_size, seq_len)\n",
    "\n",
    "    # Reshape cos and sin for broadcasting\n",
    "    # (seq_len, head_dim) -> (1, 1, seq_len, head_dim)\n",
    "    cos = cos[position_ids].unsqueeze(1)\n",
    "    sin = sin[position_ids].unsqueeze(1)\n",
    "\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "# --- 2. ALiBi (Attention with Linear Biases) Implementation ---\n",
    "\n",
    "def get_alibi_biases(num_heads, seq_len, dtype=torch.float32, device='cpu'):\n",
    "    # This generates the ALiBi slopes\n",
    "    # For a detailed explanation, see: https://ofir.io/train_with_alibi/\n",
    "    # Or original paper: https://arxiv.org/abs/2108.12409\n",
    "\n",
    "    # The slopes are usually powers of 2.\n",
    "    # We choose slopes such that they are spread across the log space.\n",
    "    def get_slopes(n):\n",
    "        def get_base(n_heads):\n",
    "            # Base for geometric series for the slopes\n",
    "            return math.pow(2, math.pow(2, -(math.log2(n_heads) - 3)))\n",
    "\n",
    "        # Get the slope for each head\n",
    "        m = get_base(n)\n",
    "        slopes = [m * (m ** (i / (n - 1))) for i in range(n)]\n",
    "        return torch.tensor(slopes, dtype=dtype, device=device)\n",
    "\n",
    "    slopes = get_slopes(num_heads) # (num_heads,)\n",
    "\n",
    "    # Create the distance matrix for ALiBi\n",
    "    # (seq_len, seq_len)\n",
    "    # Example: [[0, 1, 2],\n",
    "    #           [-1, 0, 1],\n",
    "    #           [-2, -1, 0]]\n",
    "    distance_matrix = torch.arange(seq_len, device=device).unsqueeze(0) - \\\n",
    "                      torch.arange(seq_len, device=device).unsqueeze(1) # (seq_len, seq_len)\n",
    "\n",
    "    # Expand slopes to match the attention matrix shape\n",
    "    # (num_heads, 1, 1) * (1, seq_len, seq_len) -> (num_heads, seq_len, seq_len)\n",
    "    alibi_biases = slopes.unsqueeze(-1).unsqueeze(-1) * distance_matrix.unsqueeze(0)\n",
    "    \n",
    "    # We typically want negative biases for longer distances, so attention is penalized.\n",
    "    # The ALiBi paper suggests negative slopes for Q-K.\n",
    "    # Our distance_matrix is already set up to produce negative values for relative positions\n",
    "    # (query index - key index) where query is before key.\n",
    "    # We can negate the slopes if we want positive values for shorter distances (closer to 0)\n",
    "    # and more negative for longer distances.\n",
    "    # The typical formulation in ALiBi is to add a bias based on `i - j`, where `i` is query position and `j` is key position.\n",
    "    # If `i < j`, `i - j` is negative. We want to penalize `i - j` further when it's more negative (longer distance to the right).\n",
    "    # If `i > j`, `i - j` is positive. We want to penalize `i - j` further when it's more positive (longer distance to the left).\n",
    "    # This means the slope should be negative. Let's make sure our slopes are negative.\n",
    "    alibi_biases = -torch.abs(alibi_biases) # Ensure biases are negative for a decay effect.\n",
    "    return alibi_biases\n",
    "\n",
    "# --- 3. Custom GPT2Attention with RoPE and ALiBi ---\n",
    "\n",
    "class CustomGPT2Attention(GPT2Attention):\n",
    "    def __init__(self, config, is_cross_attention=False, layer_idx=None):\n",
    "        super().__init__(config, is_cross_attention, layer_idx)\n",
    "        # Disable original positional embeddings if they are used by parent class,\n",
    "        # though GPT2Attention itself doesn't directly use them for Q/K/V.\n",
    "        # It's more about the GPT2Model's input embeddings.\n",
    "\n",
    "        # We'll need to pass seq_len to RoPE, so `_attn` method needs modification.\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        self.register_buffer(\"cos_cached\", None, persistent=False)\n",
    "        self.register_buffer(\"sin_cached\", None, persistent=False)\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "\n",
    "    def _update_cos_sin_caches(self, seq_len, device, dtype):\n",
    "        # Generate RoPE cos and sin waves\n",
    "        if self.cos_cached is not None and seq_len <= self.cos_cached.shape[-2]:\n",
    "            return\n",
    "\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, self.head_dim, 2, device=device).float() / self.head_dim))\n",
    "        t = torch.arange(seq_len, device=device, dtype=torch.float32)\n",
    "        freqs = torch.einsum(\"i,j->ij\", t, inv_freq)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1) # (seq_len, head_dim)\n",
    "        self.cos_cached = emb.cos().to(dtype)\n",
    "        self.sin_cached = emb.sin().to(dtype)\n",
    "        logger.info(f\"Updated RoPE caches for seq_len={seq_len}\")\n",
    "\n",
    "    def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n",
    "        # query, key, value: (batch_size * num_heads, seq_len, head_dim)\n",
    "\n",
    "        # Apply RoPE\n",
    "        batch_size, num_heads, seq_len, head_dim = query.shape\n",
    "        position_ids = torch.arange(seq_len, device=query.device).unsqueeze(0).repeat(batch_size, 1) # (batch_size, seq_len)\n",
    "        \n",
    "        self._update_cos_sin_caches(seq_len, query.device, query.dtype)\n",
    "\n",
    "        query, key = apply_rotary_pos_emb(\n",
    "            query, key,\n",
    "            self.cos_cached[:seq_len, :].unsqueeze(0).unsqueeze(0), # (1, 1, seq_len, head_dim)\n",
    "            self.sin_cached[:seq_len, :].unsqueeze(0).unsqueeze(0), # (1, 1, seq_len, head_dim)\n",
    "            position_ids\n",
    "        )\n",
    "\n",
    "        # Calculate attention scores\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attn_weights = attn_weights / (value.size(-1) ** 0.5)\n",
    "\n",
    "        # Apply ALiBi bias\n",
    "        # (batch_size, num_heads, seq_len, seq_len)\n",
    "        alibi_bias = get_alibi_biases(self.num_heads, seq_len, dtype=attn_weights.dtype, device=attn_weights.device)\n",
    "        attn_weights = attn_weights + alibi_bias.unsqueeze(0) # Unsqueeze batch dim for broadcasting\n",
    "\n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            # attention_mask: (batch_size, 1, 1, seq_len) or (batch_size, 1, seq_len, seq_len)\n",
    "            # Add a large negative value to masked positions\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # Apply head_mask\n",
    "        if head_mask is not None:\n",
    "            attn_weights = attn_weights * head_mask\n",
    "\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # Apply attention dropout\n",
    "        attn_weights = self.attn_dropout(attn_weights)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value)\n",
    "\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "# --- 4. Model Modification Helper Function ---\n",
    "\n",
    "def replace_attention_with_custom(model):\n",
    "    for i, block in enumerate(model.transformer.h):\n",
    "        if isinstance(block.attn, GPT2Attention):\n",
    "            logger.info(f\"Replacing GPT2Attention in block {i} with CustomGPT2Attention.\")\n",
    "            block.attn = CustomGPT2Attention(model.config, layer_idx=i)\n",
    "        else:\n",
    "            logger.warning(f\"Block {i} attention is not GPT2Attention, skipping replacement.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34224168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Dataset and Dataloader ---\n",
    "\n",
    "\n",
    "\n",
    "class HindiTextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokenizer, file_path, block_size):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.examples = []\n",
    "\n",
    "        logger.info(f\"Loading dataset from {file_path}\")\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        tokenized_text = self.tokenizer(text, return_attention_mask=False, truncation=False)[\"input_ids\"]\n",
    "        \n",
    "        # Split into blocks\n",
    "        for i in range(0, len(tokenized_text) - block_size + 1, block_size):\n",
    "            self.examples.append(tokenized_text[i : i + block_size])\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.examples)} examples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # For language modeling, input and labels are the same, shifted.\n",
    "        # The GPT2LMHeadModel internally handles the shifting.\n",
    "        return torch.tensor(self.examples[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b7495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # --- 6. Training Script ---\n",
    "\n",
    "# def train_model(\n",
    "#     tokenizer_path: str,\n",
    "#     data_file_path: str,\n",
    "#     output_dir: str,\n",
    "#     model_name_or_path: str = \"gpt2\",\n",
    "#     block_size: int = 512,\n",
    "#     batch_size: int = 8,\n",
    "#     gradient_accumulation_steps: int = 2,\n",
    "#     num_epochs: int = 3,\n",
    "#     learning_rate: float = 5e-5,\n",
    "#     warmup_steps: int = 500,\n",
    "#     max_grad_norm: float = 1.0,\n",
    "#     eval_steps: int = 1000,\n",
    "#     save_steps: int = 5000,\n",
    "# ):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     logger.info(f\"Using device: {device}\")\n",
    "\n",
    "#     # Load tokenizer\n",
    "#     # tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "#     tokenizer = T5Tokenizer.from_pretrained(\"/home/dixit/Project/May try/my_hindi_t5_tokenizer\")\n",
    "#     if tokenizer.pad_token is None:\n",
    "#         tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "#         logger.info(\"Added [PAD] token to tokenizer.\")\n",
    "\n",
    "#     # Load GPT-2 config and model\n",
    "#     config = GPT2Config.from_pretrained(model_name_or_path)\n",
    "#     # Important: Disable default positional embeddings in GPT2Model if it's explicitly adding them.\n",
    "#     # For GPT2, it's typically handled by `transformer.wpe` (word position embeddings).\n",
    "#     # We will let wpe exist, but our custom attention should handle positional information.\n",
    "#     # The key is that `GPT2Attention` itself doesn't directly use `position_ids` for its QKV calculation,\n",
    "#     # it receives already-processed `hidden_states`. Our RoPE will operate on these `hidden_states` as Q/K.\n",
    "\n",
    "#     model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config)\n",
    "    \n",
    "#     # Resize token embeddings if PAD token was added\n",
    "#     if len(tokenizer) != model.config.vocab_size:\n",
    "#         model.resize_token_embeddings(len(tokenizer))\n",
    "#         logger.info(f\"Resized model embeddings to {len(tokenizer)} vocabulary size.\")\n",
    "\n",
    "#     # Replace attention mechanisms\n",
    "#     model = replace_attention_with_custom(model)\n",
    "#     model.to(device)\n",
    "\n",
    "#     # Prepare dataset and dataloader\n",
    "#     dataset = HindiTextDataset(tokenizer=tokenizer, file_path=data_file_path, block_size=block_size)\n",
    "#     dataloader = torch.utils.data.DataLoader(\n",
    "#         dataset,\n",
    "#         batch_size=batch_size,\n",
    "#         shuffle=True,\n",
    "#         num_workers=4, # Adjust based on your system\n",
    "#         pin_memory=True if torch.cuda.is_available() else False\n",
    "#     )\n",
    "\n",
    "#     # Optimizer and Scheduler\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "#     num_training_steps = len(dataloader) // gradient_accumulation_steps * num_epochs\n",
    "#     # scheduler = torch.optim.lr_scheduler.get_linear_schedule_with_warmup(\n",
    "#     scheduler = get_linear_schedule_with_warmup(\n",
    "#         optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n",
    "#     )\n",
    "\n",
    "#     # Training loop\n",
    "#     model.train()\n",
    "#     global_step = 0\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     logger.info(\"Starting training...\")\n",
    "#     for epoch in range(num_epochs):\n",
    "#         for step, batch in enumerate(dataloader):\n",
    "#             inputs = batch.to(device)\n",
    "#             labels = inputs.clone() # For standard LM, labels are shifted inputs\n",
    "\n",
    "#             outputs = model(input_ids=inputs, labels=labels)\n",
    "#             loss = outputs.loss / gradient_accumulation_steps\n",
    "#             loss.backward()\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#             if (step + 1) % gradient_accumulation_steps == 0:\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "#                 optimizer.step()\n",
    "#                 scheduler.step()\n",
    "#                 optimizer.zero_grad()\n",
    "#                 global_step += 1\n",
    "\n",
    "#                 if global_step % 100 == 0:\n",
    "#                     logger.info(f\"Epoch {epoch+1}, Step {step+1}/{len(dataloader)}, Global Step {global_step}, Loss: {total_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "#                     total_loss = 0.0 # Reset total loss for logging\n",
    "\n",
    "#                 if global_step % save_steps == 0:\n",
    "#                     output_model_path = f\"{output_dir}/checkpoint-{global_step}\"\n",
    "#                     model.save_pretrained(output_model_path)\n",
    "#                     tokenizer.save_pretrained(output_model_path)\n",
    "#                     logger.info(f\"Model saved to {output_model_path}\")\n",
    "\n",
    "#     logger.info(\"Training complete!\")\n",
    "#     model.save_pretrained(f\"{output_dir}/final_model\")\n",
    "#     tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "#     logger.info(f\"Final model saved to {output_dir}/final_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a28fb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:datasets:PyTorch version 2.7.0 available.\n",
      "INFO:datasets:TensorFlow version 2.19.0 available.\n",
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Tokenizer already has pad_token: <pad>\n",
      "INFO:__main__:Model config: GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"dropout\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 256,\n",
      "  \"n_embd\": 512,\n",
      "  \"n_head\": 8,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 6,\n",
      "  \"n_positions\": 256,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": \"tanh\",\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 30100\n",
      "}\n",
      "\n",
      "INFO:__main__:Model embeddings vocabulary size (30100) matches tokenizer (30100).\n",
      "INFO:__main__:Replacing GPT2Attention in block 0 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 1 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 2 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 3 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 4 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 5 with CustomGPT2Attention.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45deb21322054cb09360f787d43bfa25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=8):   0%|          | 0/3669059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_60723/2125098702.py:127: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "INFO:__main__:Starting training with Hugging Face Trainer...\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1523304' max='1528770' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1523304/1528770 88:48:13 < 19:07, 4.76 it/s, Epoch 9.96/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.263600</td>\n",
       "      <td>5.216003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>4.842500</td>\n",
       "      <td>4.828564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>4.624400</td>\n",
       "      <td>4.618337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>4.488900</td>\n",
       "      <td>4.481425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>4.384300</td>\n",
       "      <td>4.376651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>4.304700</td>\n",
       "      <td>4.292759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>4.252900</td>\n",
       "      <td>4.223422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>4.213200</td>\n",
       "      <td>4.169271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>4.153000</td>\n",
       "      <td>4.111844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>4.100600</td>\n",
       "      <td>4.061904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>4.063900</td>\n",
       "      <td>4.025790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>4.041800</td>\n",
       "      <td>3.992173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>4.021700</td>\n",
       "      <td>3.955921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>3.992700</td>\n",
       "      <td>3.924858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>3.970200</td>\n",
       "      <td>3.898054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>3.945800</td>\n",
       "      <td>3.873376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>3.917900</td>\n",
       "      <td>3.850302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90000</td>\n",
       "      <td>3.923400</td>\n",
       "      <td>3.830283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95000</td>\n",
       "      <td>3.905800</td>\n",
       "      <td>3.810195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100000</td>\n",
       "      <td>3.856400</td>\n",
       "      <td>3.789488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105000</td>\n",
       "      <td>3.855900</td>\n",
       "      <td>3.769038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110000</td>\n",
       "      <td>3.857900</td>\n",
       "      <td>3.748715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115000</td>\n",
       "      <td>3.824100</td>\n",
       "      <td>3.734765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120000</td>\n",
       "      <td>3.793000</td>\n",
       "      <td>3.712608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125000</td>\n",
       "      <td>3.792200</td>\n",
       "      <td>3.701954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130000</td>\n",
       "      <td>3.797300</td>\n",
       "      <td>3.681634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135000</td>\n",
       "      <td>3.788100</td>\n",
       "      <td>3.669334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140000</td>\n",
       "      <td>3.757200</td>\n",
       "      <td>3.662258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145000</td>\n",
       "      <td>3.764600</td>\n",
       "      <td>3.647809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150000</td>\n",
       "      <td>3.743700</td>\n",
       "      <td>3.632981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155000</td>\n",
       "      <td>3.669000</td>\n",
       "      <td>3.622844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160000</td>\n",
       "      <td>3.666700</td>\n",
       "      <td>3.604778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165000</td>\n",
       "      <td>3.675600</td>\n",
       "      <td>3.592990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170000</td>\n",
       "      <td>3.668000</td>\n",
       "      <td>3.588109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175000</td>\n",
       "      <td>3.662200</td>\n",
       "      <td>3.574934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180000</td>\n",
       "      <td>3.671000</td>\n",
       "      <td>3.561439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185000</td>\n",
       "      <td>3.652300</td>\n",
       "      <td>3.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190000</td>\n",
       "      <td>3.627400</td>\n",
       "      <td>3.547988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195000</td>\n",
       "      <td>3.635000</td>\n",
       "      <td>3.542167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200000</td>\n",
       "      <td>3.620300</td>\n",
       "      <td>3.527230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205000</td>\n",
       "      <td>3.631500</td>\n",
       "      <td>3.522016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210000</td>\n",
       "      <td>3.622000</td>\n",
       "      <td>3.516801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215000</td>\n",
       "      <td>3.613400</td>\n",
       "      <td>3.509226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220000</td>\n",
       "      <td>3.610900</td>\n",
       "      <td>3.505650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225000</td>\n",
       "      <td>3.603600</td>\n",
       "      <td>3.493570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230000</td>\n",
       "      <td>3.590900</td>\n",
       "      <td>3.486278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235000</td>\n",
       "      <td>3.589700</td>\n",
       "      <td>3.481586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240000</td>\n",
       "      <td>3.581800</td>\n",
       "      <td>3.468032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245000</td>\n",
       "      <td>3.582600</td>\n",
       "      <td>3.468744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250000</td>\n",
       "      <td>3.582900</td>\n",
       "      <td>3.459708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255000</td>\n",
       "      <td>3.595100</td>\n",
       "      <td>3.457511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260000</td>\n",
       "      <td>3.551300</td>\n",
       "      <td>3.444922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265000</td>\n",
       "      <td>3.575000</td>\n",
       "      <td>3.434710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270000</td>\n",
       "      <td>3.569800</td>\n",
       "      <td>3.426752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275000</td>\n",
       "      <td>3.558600</td>\n",
       "      <td>3.423537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280000</td>\n",
       "      <td>3.558400</td>\n",
       "      <td>3.417685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285000</td>\n",
       "      <td>3.562700</td>\n",
       "      <td>3.412338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290000</td>\n",
       "      <td>3.550200</td>\n",
       "      <td>3.403639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295000</td>\n",
       "      <td>3.564200</td>\n",
       "      <td>3.400612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300000</td>\n",
       "      <td>3.537400</td>\n",
       "      <td>3.394448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305000</td>\n",
       "      <td>3.520100</td>\n",
       "      <td>3.392283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310000</td>\n",
       "      <td>3.457500</td>\n",
       "      <td>3.379041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315000</td>\n",
       "      <td>3.459300</td>\n",
       "      <td>3.371128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320000</td>\n",
       "      <td>3.460800</td>\n",
       "      <td>3.365341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325000</td>\n",
       "      <td>3.464700</td>\n",
       "      <td>3.362170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330000</td>\n",
       "      <td>3.472400</td>\n",
       "      <td>3.362339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335000</td>\n",
       "      <td>3.472100</td>\n",
       "      <td>3.357365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340000</td>\n",
       "      <td>3.481900</td>\n",
       "      <td>3.349252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345000</td>\n",
       "      <td>3.464800</td>\n",
       "      <td>3.358365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350000</td>\n",
       "      <td>3.469600</td>\n",
       "      <td>3.352089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355000</td>\n",
       "      <td>3.464700</td>\n",
       "      <td>3.343665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360000</td>\n",
       "      <td>3.473100</td>\n",
       "      <td>3.338384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365000</td>\n",
       "      <td>3.471400</td>\n",
       "      <td>3.333148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370000</td>\n",
       "      <td>3.474300</td>\n",
       "      <td>3.327367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375000</td>\n",
       "      <td>3.478900</td>\n",
       "      <td>3.317073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380000</td>\n",
       "      <td>3.456800</td>\n",
       "      <td>3.315487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>385000</td>\n",
       "      <td>3.467600</td>\n",
       "      <td>3.312714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390000</td>\n",
       "      <td>3.440500</td>\n",
       "      <td>3.303314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>395000</td>\n",
       "      <td>3.454900</td>\n",
       "      <td>3.304353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400000</td>\n",
       "      <td>3.446800</td>\n",
       "      <td>3.294730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405000</td>\n",
       "      <td>3.462700</td>\n",
       "      <td>3.294121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410000</td>\n",
       "      <td>3.442900</td>\n",
       "      <td>3.288481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>415000</td>\n",
       "      <td>3.444100</td>\n",
       "      <td>3.287279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420000</td>\n",
       "      <td>3.429300</td>\n",
       "      <td>3.281701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425000</td>\n",
       "      <td>3.441600</td>\n",
       "      <td>3.271735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430000</td>\n",
       "      <td>3.427200</td>\n",
       "      <td>3.272989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435000</td>\n",
       "      <td>3.434000</td>\n",
       "      <td>3.269181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440000</td>\n",
       "      <td>3.434500</td>\n",
       "      <td>3.265518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>445000</td>\n",
       "      <td>3.428900</td>\n",
       "      <td>3.261455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450000</td>\n",
       "      <td>3.409400</td>\n",
       "      <td>3.262542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>455000</td>\n",
       "      <td>3.420700</td>\n",
       "      <td>3.256483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460000</td>\n",
       "      <td>3.361800</td>\n",
       "      <td>3.240882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>465000</td>\n",
       "      <td>3.366700</td>\n",
       "      <td>3.236579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470000</td>\n",
       "      <td>3.365600</td>\n",
       "      <td>3.237743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475000</td>\n",
       "      <td>3.365300</td>\n",
       "      <td>3.236192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480000</td>\n",
       "      <td>3.342900</td>\n",
       "      <td>3.235934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>485000</td>\n",
       "      <td>3.362600</td>\n",
       "      <td>3.227272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490000</td>\n",
       "      <td>3.377300</td>\n",
       "      <td>3.229761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>495000</td>\n",
       "      <td>3.351500</td>\n",
       "      <td>3.219268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500000</td>\n",
       "      <td>3.362500</td>\n",
       "      <td>3.221698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>505000</td>\n",
       "      <td>3.367100</td>\n",
       "      <td>3.210951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510000</td>\n",
       "      <td>3.352500</td>\n",
       "      <td>3.215791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>515000</td>\n",
       "      <td>3.363100</td>\n",
       "      <td>3.210175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520000</td>\n",
       "      <td>3.364200</td>\n",
       "      <td>3.214116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>525000</td>\n",
       "      <td>3.359600</td>\n",
       "      <td>3.205081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530000</td>\n",
       "      <td>3.367000</td>\n",
       "      <td>3.206594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>535000</td>\n",
       "      <td>3.355500</td>\n",
       "      <td>3.201300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540000</td>\n",
       "      <td>3.354100</td>\n",
       "      <td>3.204843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>545000</td>\n",
       "      <td>3.361100</td>\n",
       "      <td>3.198345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550000</td>\n",
       "      <td>3.363800</td>\n",
       "      <td>3.194154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>555000</td>\n",
       "      <td>3.343500</td>\n",
       "      <td>3.191887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560000</td>\n",
       "      <td>3.350600</td>\n",
       "      <td>3.193929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>565000</td>\n",
       "      <td>3.355800</td>\n",
       "      <td>3.190783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570000</td>\n",
       "      <td>3.358100</td>\n",
       "      <td>3.190098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>575000</td>\n",
       "      <td>3.361500</td>\n",
       "      <td>3.185222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580000</td>\n",
       "      <td>3.354000</td>\n",
       "      <td>3.184751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>585000</td>\n",
       "      <td>3.342000</td>\n",
       "      <td>3.187198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590000</td>\n",
       "      <td>3.343200</td>\n",
       "      <td>3.184240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>595000</td>\n",
       "      <td>3.365000</td>\n",
       "      <td>3.170658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600000</td>\n",
       "      <td>3.338100</td>\n",
       "      <td>3.174099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>605000</td>\n",
       "      <td>3.335900</td>\n",
       "      <td>3.169430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610000</td>\n",
       "      <td>3.337800</td>\n",
       "      <td>3.165839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>615000</td>\n",
       "      <td>3.283100</td>\n",
       "      <td>3.150762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620000</td>\n",
       "      <td>3.279300</td>\n",
       "      <td>3.148058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>625000</td>\n",
       "      <td>3.284000</td>\n",
       "      <td>3.147617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630000</td>\n",
       "      <td>3.274000</td>\n",
       "      <td>3.145785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>635000</td>\n",
       "      <td>3.286200</td>\n",
       "      <td>3.144368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640000</td>\n",
       "      <td>3.294200</td>\n",
       "      <td>3.143843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>645000</td>\n",
       "      <td>3.291900</td>\n",
       "      <td>3.144097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650000</td>\n",
       "      <td>3.275300</td>\n",
       "      <td>3.140562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>655000</td>\n",
       "      <td>3.289200</td>\n",
       "      <td>3.141288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660000</td>\n",
       "      <td>3.288700</td>\n",
       "      <td>3.135251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>665000</td>\n",
       "      <td>3.301000</td>\n",
       "      <td>3.130453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670000</td>\n",
       "      <td>3.280800</td>\n",
       "      <td>3.122842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>675000</td>\n",
       "      <td>3.307100</td>\n",
       "      <td>3.123480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680000</td>\n",
       "      <td>3.292900</td>\n",
       "      <td>3.123229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>685000</td>\n",
       "      <td>3.284700</td>\n",
       "      <td>3.119452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690000</td>\n",
       "      <td>3.298400</td>\n",
       "      <td>3.120180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>695000</td>\n",
       "      <td>3.302500</td>\n",
       "      <td>3.117132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700000</td>\n",
       "      <td>3.295400</td>\n",
       "      <td>3.111965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>705000</td>\n",
       "      <td>3.291700</td>\n",
       "      <td>3.114610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710000</td>\n",
       "      <td>3.274400</td>\n",
       "      <td>3.110270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715000</td>\n",
       "      <td>3.271000</td>\n",
       "      <td>3.112005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720000</td>\n",
       "      <td>3.261300</td>\n",
       "      <td>3.100577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>725000</td>\n",
       "      <td>3.280600</td>\n",
       "      <td>3.104182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730000</td>\n",
       "      <td>3.279300</td>\n",
       "      <td>3.105619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>735000</td>\n",
       "      <td>3.292500</td>\n",
       "      <td>3.104071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740000</td>\n",
       "      <td>3.286300</td>\n",
       "      <td>3.090931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>745000</td>\n",
       "      <td>3.280800</td>\n",
       "      <td>3.095872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750000</td>\n",
       "      <td>3.284000</td>\n",
       "      <td>3.086491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>755000</td>\n",
       "      <td>3.277700</td>\n",
       "      <td>3.083702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760000</td>\n",
       "      <td>3.278900</td>\n",
       "      <td>3.084804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>765000</td>\n",
       "      <td>3.222200</td>\n",
       "      <td>3.076163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770000</td>\n",
       "      <td>3.214800</td>\n",
       "      <td>3.077622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>775000</td>\n",
       "      <td>3.227100</td>\n",
       "      <td>3.080408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780000</td>\n",
       "      <td>3.225100</td>\n",
       "      <td>3.078489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>785000</td>\n",
       "      <td>3.222300</td>\n",
       "      <td>3.072125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790000</td>\n",
       "      <td>3.196700</td>\n",
       "      <td>3.074772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>795000</td>\n",
       "      <td>3.225200</td>\n",
       "      <td>3.069625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800000</td>\n",
       "      <td>3.234500</td>\n",
       "      <td>3.066820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>805000</td>\n",
       "      <td>3.239900</td>\n",
       "      <td>3.064894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810000</td>\n",
       "      <td>3.218100</td>\n",
       "      <td>3.065536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>815000</td>\n",
       "      <td>3.214000</td>\n",
       "      <td>3.067167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820000</td>\n",
       "      <td>3.228900</td>\n",
       "      <td>3.063484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>825000</td>\n",
       "      <td>3.232800</td>\n",
       "      <td>3.065698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830000</td>\n",
       "      <td>3.228300</td>\n",
       "      <td>3.060504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>835000</td>\n",
       "      <td>3.234700</td>\n",
       "      <td>3.054413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840000</td>\n",
       "      <td>3.226600</td>\n",
       "      <td>3.052083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>845000</td>\n",
       "      <td>3.232300</td>\n",
       "      <td>3.048548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850000</td>\n",
       "      <td>3.219500</td>\n",
       "      <td>3.044146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>855000</td>\n",
       "      <td>3.233100</td>\n",
       "      <td>3.045444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860000</td>\n",
       "      <td>3.245700</td>\n",
       "      <td>3.046503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>865000</td>\n",
       "      <td>3.213200</td>\n",
       "      <td>3.038217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870000</td>\n",
       "      <td>3.218500</td>\n",
       "      <td>3.037043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>875000</td>\n",
       "      <td>3.222500</td>\n",
       "      <td>3.028619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880000</td>\n",
       "      <td>3.216700</td>\n",
       "      <td>3.029078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>885000</td>\n",
       "      <td>3.233500</td>\n",
       "      <td>3.034797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890000</td>\n",
       "      <td>3.242400</td>\n",
       "      <td>3.022187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>895000</td>\n",
       "      <td>3.228300</td>\n",
       "      <td>3.020079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900000</td>\n",
       "      <td>3.206400</td>\n",
       "      <td>3.019857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>905000</td>\n",
       "      <td>3.227900</td>\n",
       "      <td>3.020683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910000</td>\n",
       "      <td>3.215000</td>\n",
       "      <td>3.016814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>915000</td>\n",
       "      <td>3.213000</td>\n",
       "      <td>3.013625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920000</td>\n",
       "      <td>3.132500</td>\n",
       "      <td>3.003273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>925000</td>\n",
       "      <td>3.164800</td>\n",
       "      <td>3.009549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930000</td>\n",
       "      <td>3.145000</td>\n",
       "      <td>3.004646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>935000</td>\n",
       "      <td>3.162900</td>\n",
       "      <td>2.999729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940000</td>\n",
       "      <td>3.169700</td>\n",
       "      <td>3.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>945000</td>\n",
       "      <td>3.158500</td>\n",
       "      <td>3.000151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950000</td>\n",
       "      <td>3.164600</td>\n",
       "      <td>2.993418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>955000</td>\n",
       "      <td>3.167600</td>\n",
       "      <td>2.992235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960000</td>\n",
       "      <td>3.177600</td>\n",
       "      <td>3.001596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>965000</td>\n",
       "      <td>3.163900</td>\n",
       "      <td>2.996248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970000</td>\n",
       "      <td>3.159700</td>\n",
       "      <td>2.998448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>975000</td>\n",
       "      <td>3.175800</td>\n",
       "      <td>2.990166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980000</td>\n",
       "      <td>3.166100</td>\n",
       "      <td>2.994159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>985000</td>\n",
       "      <td>3.165600</td>\n",
       "      <td>2.985959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990000</td>\n",
       "      <td>3.179200</td>\n",
       "      <td>2.985441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>995000</td>\n",
       "      <td>3.163700</td>\n",
       "      <td>2.985300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000000</td>\n",
       "      <td>3.173300</td>\n",
       "      <td>2.983478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1005000</td>\n",
       "      <td>3.164400</td>\n",
       "      <td>2.982662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010000</td>\n",
       "      <td>3.172100</td>\n",
       "      <td>2.976744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1015000</td>\n",
       "      <td>3.148100</td>\n",
       "      <td>2.971771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020000</td>\n",
       "      <td>3.163100</td>\n",
       "      <td>2.967719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1025000</td>\n",
       "      <td>3.164600</td>\n",
       "      <td>2.964682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030000</td>\n",
       "      <td>3.168500</td>\n",
       "      <td>2.967712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1035000</td>\n",
       "      <td>3.150100</td>\n",
       "      <td>2.965752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040000</td>\n",
       "      <td>3.164600</td>\n",
       "      <td>2.958076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1045000</td>\n",
       "      <td>3.159400</td>\n",
       "      <td>2.962819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050000</td>\n",
       "      <td>3.178800</td>\n",
       "      <td>2.959572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1055000</td>\n",
       "      <td>3.167200</td>\n",
       "      <td>2.960831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060000</td>\n",
       "      <td>3.178500</td>\n",
       "      <td>2.961999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1065000</td>\n",
       "      <td>3.165000</td>\n",
       "      <td>2.957322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070000</td>\n",
       "      <td>3.146100</td>\n",
       "      <td>2.954281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1075000</td>\n",
       "      <td>3.102500</td>\n",
       "      <td>2.942098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080000</td>\n",
       "      <td>3.115600</td>\n",
       "      <td>2.946587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1085000</td>\n",
       "      <td>3.108300</td>\n",
       "      <td>2.945966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090000</td>\n",
       "      <td>3.096800</td>\n",
       "      <td>2.944930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1095000</td>\n",
       "      <td>3.098700</td>\n",
       "      <td>2.947367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100000</td>\n",
       "      <td>3.112900</td>\n",
       "      <td>2.941522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1105000</td>\n",
       "      <td>3.099900</td>\n",
       "      <td>2.936184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110000</td>\n",
       "      <td>3.118000</td>\n",
       "      <td>2.935450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1115000</td>\n",
       "      <td>3.106000</td>\n",
       "      <td>2.930269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120000</td>\n",
       "      <td>3.115200</td>\n",
       "      <td>2.927693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1125000</td>\n",
       "      <td>3.109000</td>\n",
       "      <td>2.928137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130000</td>\n",
       "      <td>3.110000</td>\n",
       "      <td>2.925307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1135000</td>\n",
       "      <td>3.107200</td>\n",
       "      <td>2.920890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140000</td>\n",
       "      <td>3.102600</td>\n",
       "      <td>2.918158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1145000</td>\n",
       "      <td>3.118700</td>\n",
       "      <td>2.915437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150000</td>\n",
       "      <td>3.114600</td>\n",
       "      <td>2.919543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1155000</td>\n",
       "      <td>3.120100</td>\n",
       "      <td>2.911855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160000</td>\n",
       "      <td>3.115500</td>\n",
       "      <td>2.919278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1165000</td>\n",
       "      <td>3.109800</td>\n",
       "      <td>2.916766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170000</td>\n",
       "      <td>3.109500</td>\n",
       "      <td>2.908145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1175000</td>\n",
       "      <td>3.124300</td>\n",
       "      <td>2.906456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180000</td>\n",
       "      <td>3.116700</td>\n",
       "      <td>2.911317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1185000</td>\n",
       "      <td>3.121200</td>\n",
       "      <td>2.908418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190000</td>\n",
       "      <td>3.100600</td>\n",
       "      <td>2.906504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1195000</td>\n",
       "      <td>3.115500</td>\n",
       "      <td>2.901425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200000</td>\n",
       "      <td>3.106200</td>\n",
       "      <td>2.899790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1205000</td>\n",
       "      <td>3.094500</td>\n",
       "      <td>2.896130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210000</td>\n",
       "      <td>3.103300</td>\n",
       "      <td>2.896425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1215000</td>\n",
       "      <td>3.108500</td>\n",
       "      <td>2.895231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220000</td>\n",
       "      <td>3.115000</td>\n",
       "      <td>2.891013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1225000</td>\n",
       "      <td>3.047200</td>\n",
       "      <td>2.884714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230000</td>\n",
       "      <td>3.040600</td>\n",
       "      <td>2.884229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1235000</td>\n",
       "      <td>3.049400</td>\n",
       "      <td>2.881754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240000</td>\n",
       "      <td>3.047800</td>\n",
       "      <td>2.877707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1245000</td>\n",
       "      <td>3.061900</td>\n",
       "      <td>2.879836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250000</td>\n",
       "      <td>3.054500</td>\n",
       "      <td>2.876818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1255000</td>\n",
       "      <td>3.047300</td>\n",
       "      <td>2.871747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260000</td>\n",
       "      <td>3.055100</td>\n",
       "      <td>2.867181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1265000</td>\n",
       "      <td>3.060000</td>\n",
       "      <td>2.871278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270000</td>\n",
       "      <td>3.049200</td>\n",
       "      <td>2.870412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1275000</td>\n",
       "      <td>3.043700</td>\n",
       "      <td>2.866730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280000</td>\n",
       "      <td>3.054600</td>\n",
       "      <td>2.864429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1285000</td>\n",
       "      <td>3.068400</td>\n",
       "      <td>2.864760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290000</td>\n",
       "      <td>3.059200</td>\n",
       "      <td>2.865140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1295000</td>\n",
       "      <td>3.060200</td>\n",
       "      <td>2.864563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300000</td>\n",
       "      <td>3.060900</td>\n",
       "      <td>2.864927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1305000</td>\n",
       "      <td>3.051600</td>\n",
       "      <td>2.852452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310000</td>\n",
       "      <td>3.068500</td>\n",
       "      <td>2.859628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1315000</td>\n",
       "      <td>3.047400</td>\n",
       "      <td>2.856675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320000</td>\n",
       "      <td>3.064800</td>\n",
       "      <td>2.852329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1325000</td>\n",
       "      <td>3.051300</td>\n",
       "      <td>2.854044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330000</td>\n",
       "      <td>3.046100</td>\n",
       "      <td>2.847725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1335000</td>\n",
       "      <td>3.050700</td>\n",
       "      <td>2.844982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340000</td>\n",
       "      <td>3.054300</td>\n",
       "      <td>2.843379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1345000</td>\n",
       "      <td>3.048700</td>\n",
       "      <td>2.842337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350000</td>\n",
       "      <td>3.060200</td>\n",
       "      <td>2.840328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1355000</td>\n",
       "      <td>3.036300</td>\n",
       "      <td>2.839214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360000</td>\n",
       "      <td>3.061300</td>\n",
       "      <td>2.838506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1365000</td>\n",
       "      <td>3.032700</td>\n",
       "      <td>2.836469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370000</td>\n",
       "      <td>3.050500</td>\n",
       "      <td>2.837290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1375000</td>\n",
       "      <td>3.057800</td>\n",
       "      <td>2.833392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380000</td>\n",
       "      <td>2.990700</td>\n",
       "      <td>2.826940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1385000</td>\n",
       "      <td>2.982400</td>\n",
       "      <td>2.827082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390000</td>\n",
       "      <td>3.009900</td>\n",
       "      <td>2.825791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1395000</td>\n",
       "      <td>3.001100</td>\n",
       "      <td>2.824951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400000</td>\n",
       "      <td>3.006700</td>\n",
       "      <td>2.821598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1405000</td>\n",
       "      <td>2.992400</td>\n",
       "      <td>2.821651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410000</td>\n",
       "      <td>2.988800</td>\n",
       "      <td>2.819220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1415000</td>\n",
       "      <td>2.987300</td>\n",
       "      <td>2.813676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420000</td>\n",
       "      <td>3.011500</td>\n",
       "      <td>2.816203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1425000</td>\n",
       "      <td>2.992100</td>\n",
       "      <td>2.813442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430000</td>\n",
       "      <td>3.012700</td>\n",
       "      <td>2.815396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1435000</td>\n",
       "      <td>3.002800</td>\n",
       "      <td>2.812053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440000</td>\n",
       "      <td>3.004500</td>\n",
       "      <td>2.811884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1445000</td>\n",
       "      <td>2.980100</td>\n",
       "      <td>2.814093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450000</td>\n",
       "      <td>3.005800</td>\n",
       "      <td>2.810934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1455000</td>\n",
       "      <td>2.990200</td>\n",
       "      <td>2.810130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460000</td>\n",
       "      <td>3.000800</td>\n",
       "      <td>2.807585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1465000</td>\n",
       "      <td>2.988000</td>\n",
       "      <td>2.808325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470000</td>\n",
       "      <td>2.983600</td>\n",
       "      <td>2.804214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1475000</td>\n",
       "      <td>2.988800</td>\n",
       "      <td>2.801629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480000</td>\n",
       "      <td>2.974400</td>\n",
       "      <td>2.803074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1485000</td>\n",
       "      <td>3.004000</td>\n",
       "      <td>2.800953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490000</td>\n",
       "      <td>3.002700</td>\n",
       "      <td>2.800135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1495000</td>\n",
       "      <td>2.978200</td>\n",
       "      <td>2.797858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500000</td>\n",
       "      <td>2.993200</td>\n",
       "      <td>2.797378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1505000</td>\n",
       "      <td>2.986200</td>\n",
       "      <td>2.797879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510000</td>\n",
       "      <td>2.979500</td>\n",
       "      <td>2.796255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1515000</td>\n",
       "      <td>2.987100</td>\n",
       "      <td>2.795997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520000</td>\n",
       "      <td>2.998200</td>\n",
       "      <td>2.795945</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ... (imports and other functions remain the same, including CustomGPT2Attention, etc.) ...\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling # Import these\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "seed = 42\n",
    "# The `train_model` function will be significantly simpler\n",
    "def train_model_with_trainer(\n",
    "    tokenizer_path: str,\n",
    "    data_file_path: str,\n",
    "    output_dir: str,\n",
    "    custom_config: Optional[GPT2Config] = None,\n",
    "    custom_training_args: Optional[TrainingArguments] = None, # New argument for TrainingArguments\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    logger.info(f\"Using device: {device}\")\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"/home/dixit/Project/May try/my_hindi_t5_tokenizer\")\n",
    "    tokenizer.model_max_length = 256\n",
    "    tokenizer.pad_token = \"<pad>\"  \n",
    "    tokenizer.padding_size = \"right\"\n",
    "    if tokenizer.pad_token is None:\n",
    "        if \"[PAD]\" in tokenizer.vocab:\n",
    "            tokenizer.pad_token = \"[PAD]\"\n",
    "        elif \"<pad>\" in tokenizer.vocab:\n",
    "            tokenizer.pad_token = \"<pad>\"\n",
    "        elif tokenizer.unk_token is not None:\n",
    "            tokenizer.pad_token = tokenizer.unk_token\n",
    "            logger.warning(f\"Using tokenizer.unk_token ({tokenizer.unk_token}) as pad_token. Consider training with a dedicated pad_token.\")\n",
    "        else:\n",
    "            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "            logger.info(\"Added new [PAD] token to tokenizer and set it as pad_token.\")\n",
    "    else:\n",
    "        logger.info(f\"Tokenizer already has pad_token: {tokenizer.pad_token}\")\n",
    "\n",
    "\n",
    "    # Load or create GPT-2 config\n",
    "    if custom_config:\n",
    "        config = custom_config\n",
    "        config.vocab_size = len(tokenizer)\n",
    "        if tokenizer.bos_token_id is not None: config.bos_token_id = tokenizer.bos_token_id\n",
    "        if tokenizer.eos_token_id is not None: config.eos_token_id = tokenizer.eos_token_id\n",
    "        if tokenizer.pad_token_id is not None: config.pad_token_id = tokenizer.pad_token_id\n",
    "    else:\n",
    "        config = GPT2Config.from_pretrained(\"gpt2\")\n",
    "        config.vocab_size = len(tokenizer)\n",
    "        if tokenizer.bos_token_id is not None: config.bos_token_id = tokenizer.bos_token_id\n",
    "        if tokenizer.eos_token_id is not None: config.eos_token_id = tokenizer.eos_token_id\n",
    "        if tokenizer.pad_token_id is not None: config.pad_token_id = tokenizer.pad_token_id\n",
    "    logger.info(f\"Model config: {config}\")\n",
    "\n",
    "    model = GPT2LMHeadModel(config=config)\n",
    "    if len(tokenizer) != model.config.vocab_size:\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "        logger.info(f\"Resized model embeddings to {len(tokenizer)} vocabulary size.\")\n",
    "    else:\n",
    "        logger.info(f\"Model embeddings vocabulary size ({model.config.vocab_size}) matches tokenizer ({len(tokenizer)}).\")\n",
    "\n",
    "    # Replace attention mechanisms\n",
    "    model = replace_attention_with_custom(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # Prepare dataset\n",
    "    # For Trainer, the dataset can be simpler if DataCollator handles padding/truncation\n",
    "    # dataset = HindiTextDataset(tokenizer=tokenizer, file_path=data_file_path, block_size=config.n_ctx) # Use config.n_ctx for block_size\n",
    "    dataset = load_dataset(\"text\", data_files={\"train\": data_file_path})\n",
    "    def tokenize_function(examples):\n",
    "        # return {\"input_ids\": tokenizer.EncodeAsIds(examples[\"text\"])}\n",
    "        tokens = tokenizer(examples[\"text\"], truncation=True)\n",
    "        return {\"input_ids\": tokens[\"input_ids\"]}\n",
    "\n",
    "    print(\"tokenizing...\")\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"],\n",
    "        batch_size=500,\n",
    "        num_proc=8,  # Adjust based on CPU cores\n",
    "        load_from_cache_file=True\n",
    "    )\n",
    "    # Data Collator for Language Modeling: handles shifting labels and padding\n",
    "    # mlm=False for causal language modeling (like GPT-2)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    train_dataset_1 = tokenized_datasets[\"train\"]\n",
    "\n",
    "    # Shuffle and select a fixed subset for eval_dataset\n",
    "    eval_dataset_1 = train_dataset_1.shuffle(seed=42).select(range(2000))\n",
    "\n",
    "    # Define your TrainingArguments\n",
    "    if custom_training_args:\n",
    "        training_args = custom_training_args\n",
    "        # You might want to update output_dir and logging_dir if they are relative\n",
    "        # training_args.output_dir = os.path.join(output_dir, training_args.output_dir)\n",
    "        # training_args.logging_dir = os.path.join(output_dir, training_args.logging_dir)\n",
    "    else:\n",
    "        # Define some default TrainingArguments if not provided\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=10,\n",
    "            per_device_train_batch_size=6,  \n",
    "            per_device_eval_batch_size=2,  \n",
    "            gradient_accumulation_steps=4,  \n",
    "            learning_rate=2e-4,\n",
    "            optim=\"adafactor\",\n",
    "            max_grad_norm=1.0,\n",
    "            save_total_limit=100000,\n",
    "            logging_dir=\"/home/dixit/Project/May try/hindi_gpt2_rope_alibi_model_logs\",\n",
    "            eval_strategy =\"steps\", \n",
    "            save_strategy=\"epoch\",\n",
    "            logging_steps=1000,\n",
    "            eval_steps=5000, \n",
    "            warmup_steps=1000,\n",
    "            save_steps = 10000,\n",
    "            weight_decay=0.01,\n",
    "            dataloader_num_workers=2,\n",
    "            remove_unused_columns=False, \n",
    "            resume_from_checkpoint=True,  \n",
    "            fp16=True,  \n",
    "            eval_accumulation_steps=None,\n",
    "            report_to=\"tensorboard\",\n",
    "        )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset_1,\n",
    "        eval_dataset=eval_dataset_1,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    logger.info(\"Starting training with Hugging Face Trainer...\")\n",
    "    # trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
    "    trainer.train(resume_from_checkpoint=True)\n",
    "\n",
    "    logger.info(\"Training complete!\")\n",
    "    trainer.save_model(f\"{output_dir}/final_model\")\n",
    "    tokenizer.save_pretrained(f\"{output_dir}/final_model\")\n",
    "    logger.info(f\"Final model saved to {output_dir}/final_model\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # ... (tokenizer and data setup, including custom_config definition, remains the same) ...\n",
    "    output_dir = \"./hindi_gpt2_rope_alibi_model\"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"/home/dixit/Project/May try/my_hindi_t5_tokenizer\")\n",
    "    tokenizer.model_max_length = 256\n",
    "    tokenizer.pad_token = \"<pad>\"  \n",
    "    tokenizer.padding_size = \"right\"\n",
    "    my_config= GPT2Config(\n",
    "        vocab_size = tokenizer.vocab_size,\n",
    "        n_positions=256,\n",
    "        n_ctx=256,\n",
    "        n_embd=512,\n",
    "        n_layer=6,\n",
    "        n_head=8,\n",
    "        bos_token_id = tokenizer.bos_token_id,\n",
    "        eos_token_id = tokenizer.eos_token_id,\n",
    "        layer_norm_epsilon=1e-5,\n",
    "        initializer_range=0.02,\n",
    "        summary_activation='tanh',\n",
    "        use_cache=False,\n",
    "        dropout= 0.1,\n",
    "    )\n",
    "    # Define your TrainingArguments\n",
    "    my_training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=6,\n",
    "        per_device_eval_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        learning_rate=2e-4,\n",
    "        optim=\"adafactor\",\n",
    "        max_grad_norm=1.0,\n",
    "        save_total_limit=None, # Set to None or a specific number. 100000 is very high.\n",
    "        logging_dir=\"/home/dixit/Project/May try/hindi_gpt2_rope_alibi_model_logs\",\n",
    "        eval_strategy =\"steps\", # Corrected from eval_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_steps=1000,\n",
    "        eval_steps=5000,\n",
    "        warmup_steps=1000,\n",
    "        save_steps = 10000,\n",
    "        weight_decay=0.01,\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        resume_from_checkpoint=True,\n",
    "        fp16=True,\n",
    "        eval_accumulation_steps=None,\n",
    "        report_to=\"tensorboard\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"/home/dixit/Project/May try/my_hindi_t5_tokenizer\")\n",
    "    \n",
    "\n",
    "    # 4. Run the training function with Trainer\n",
    "    train_model_with_trainer(\n",
    "        tokenizer_path=\"/home/dixit/Project/May try/my_hindi_t5_tokenizer\",\n",
    "        data_file_path=\"/home/dixit/Project/May try/hindi_corpus_fixed.txt\",\n",
    "        output_dir=\"/home/dixit/Project/May try/hindi_gpt2_rope_alibi_model\", # This will be the base for output_dir in TrainingArguments\n",
    "        custom_config=my_config,\n",
    "        custom_training_args=my_training_args # Pass your custom training args here\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa59a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Using device: cuda\n",
      "INFO:__main__:Resized model embeddings to 30100 vocabulary size.\n",
      "INFO:__main__:Replacing GPT2Attention in block 0 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 1 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 2 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 3 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 4 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 5 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 6 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 7 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 8 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 9 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 10 with CustomGPT2Attention.\n",
      "INFO:__main__:Replacing GPT2Attention in block 11 with CustomGPT2Attention.\n",
      "INFO:__main__:Loading dataset from hindi_dummy_data.txt\n",
      "INFO:__main__:Loaded 32 examples.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for //: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     39\u001b[39m os.makedirs(output_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 4. Run the training function\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Start with base GPT-2 weights\u001b[39;49;00m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Smaller block size for demonstration\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Smaller batch size for demonstration\u001b[39;49;00m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Shorter training for demonstration\u001b[39;49;00m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwarmup_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m    \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\n\u001b[32m     55\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# After training, you can load your model and generate text:\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# from transformers import pipeline\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# model_path = f\"{output_dir}/final_model\"\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# generated_text = generator(prompt, max_length=50, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\u001b[39;00m\n\u001b[32m     72\u001b[39m \u001b[38;5;66;03m# print(generated_text)\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(tokenizer_path, data_file_path, output_dir, model_name_or_path, block_size, batch_size, gradient_accumulation_steps, num_epochs, learning_rate, warmup_steps, max_grad_norm, eval_steps, save_steps)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Optimizer and Scheduler\u001b[39;00m\n\u001b[32m     58\u001b[39m optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m num_training_steps = \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m * num_epochs\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# scheduler = torch.optim.lr_scheduler.get_linear_schedule_with_warmup(\u001b[39;00m\n\u001b[32m     61\u001b[39m scheduler = get_linear_schedule_with_warmup(\n\u001b[32m     62\u001b[39m     optimizer, num_warmup_steps=warmup_steps, num_training_steps=num_training_steps\n\u001b[32m     63\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: unsupported operand type(s) for //: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     # Create dummy data and tokenizer for demonstration\n",
    "#     # In a real scenario, you would have your pre-trained Hindi tokenizer\n",
    "#     # and a large Hindi text file.\n",
    "\n",
    "#     # 1. Prepare your Hindi tokenizer and save it\n",
    "#     # Example (replace with your actual tokenizer training):\n",
    "#     # from tokenizers import ByteLevelBPETokenizer\n",
    "#     # tokenizer = ByteLevelBPETokenizer()\n",
    "#     # tokenizer.train(files=[\"./hindi_corpus.txt\"], vocab_size=50257, min_frequency=2,\n",
    "#     #                 special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n",
    "#     # tokenizer.save_model(\"./my_hindi_tokenizer\")\n",
    "#     # tokenizer_path = \"./my_hindi_tokenizer\"\n",
    "\n",
    "#     # For demonstration, we'll use a pre-trained tokenizer and just add a dummy pad token\n",
    "#     # If you have a custom tokenizer, make sure it's saved to a directory\n",
    "#     # and provide that path.\n",
    "#     # tokenizer_path = \"google/byt5-small\" # Using a placeholder for demonstration\n",
    "#     # You would replace this with the path to your actual Hindi tokenizer\n",
    "#     # e.g., './my_hindi_tokenizer' if you saved it there.\n",
    "#     tokenizer_path = \"/home/dixit/Project/May try/my_hindi_t5_tokenizer\" \n",
    "\n",
    "#     # 2. Create a dummy Hindi text file\n",
    "#     # dummy_text = \"\"\"\n",
    "#     # ,   Gemini          \n",
    "#     #       ?          \n",
    "#     #          \n",
    "#     #         \n",
    "#     #         \n",
    "#     # \"\"\" * 100 # Repeat for more data\n",
    "\n",
    "#     # with open(\"hindi_dummy_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     #     f.write(dummy_text)\n",
    "#     data_file_path = \"hindi_dummy_data.txt\"\n",
    "\n",
    "#     # 3. Define output directory\n",
    "#     output_dir = \"./hindi_gpt2_rope_alibi_model\"\n",
    "#     import os\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "#     # 4. Run the training function\n",
    "#     train_model(\n",
    "#         tokenizer_path=tokenizer_path,\n",
    "#         data_file_path=data_file_path,\n",
    "#         output_dir=output_dir,\n",
    "#         model_name_or_path=\"gpt2\", # Start with base GPT-2 weights\n",
    "#         block_size=256, # Smaller block size for demonstration\n",
    "#         batch_size=2,   # Smaller batch size for demonstration\n",
    "#         gradient_accumulation_steps=None,\n",
    "#         num_epochs=10,   # Shorter training for demonstration\n",
    "#         learning_rate=2e-4,\n",
    "#         warmup_steps=1000,\n",
    "#         eval_steps=5000,\n",
    "#         save_steps=10000\n",
    "#     )\n",
    "\n",
    "#     # After training, you can load your model and generate text:\n",
    "#     # from transformers import pipeline\n",
    "#     # model_path = f\"{output_dir}/final_model\"\n",
    "#     # loaded_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "#     # loaded_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "#     # # Make sure to re-replace attention for the loaded model if it was saved without the custom class definition\n",
    "#     # # (though PyTorch should save the custom class definition if it's in the same file)\n",
    "#     # # loaded_model = replace_attention_with_custom(loaded_model)\n",
    "#     # loaded_model.to(device)\n",
    "#     # loaded_model.eval()\n",
    "#     #\n",
    "#     # generator = pipeline('text-generation', model=loaded_model, tokenizer=loaded_tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "#     #\n",
    "#     # prompt = \",   \"\n",
    "#     # generated_text = generator(prompt, max_length=50, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95)\n",
    "#     # print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134f551",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be562fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfa55ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hindi_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
